{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d2441db-0ad8-40f2-b042-6bb96f184711",
      "metadata": {
        "id": "8d2441db-0ad8-40f2-b042-6bb96f184711"
      },
      "source": [
        "# Rag From Scratch: Query Transformations\n",
        "\n",
        "Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
        "\n",
        "![Screenshot 2024-03-25 at 8.08.30 PM.png](attachment:d9d5305c-e5bb-4934-b91d-5988c87fd767.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5997742",
      "metadata": {
        "id": "a5997742"
      },
      "source": [
        "## Set Environment Vars and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
      "metadata": {
        "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
        "outputId": "3a7959a9-920c-4263-f5ab-f76f4ad7f212"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25607107",
      "metadata": {
        "id": "25607107"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\n",
        "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
      "metadata": {
        "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0"
      },
      "source": [
        "## Part 5: Multi Query\n",
        "\n",
        "Flow:\n",
        "\n",
        "![Screenshot 2024-02-12 at 12.39.59 PM.png](attachment:9efe017a-075f-4017-abef-174c755b11c6.png)\n",
        "\n",
        "Docs:\n",
        "\n",
        "* https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
        "\n",
        "### Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
      "metadata": {
        "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
        "outputId": "4d737a97-a439-4228-f664-6f559373a1b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Vasanth\\anaconda\\envs\\markdown-validation-crew\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "vectorstore = FAISS.from_documents(documents=splits,\n",
        "                                    embedding=hf_embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa",
      "metadata": {
        "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa"
      },
      "source": [
        "### Multi Query Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6",
      "metadata": {
        "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatGroq(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82577a7",
      "metadata": {
        "id": "a82577a7",
        "outputId": "9c6dc1df-e98c-4619-a5b4-42ff2a32be65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1. Can you explain the concept of task decomposition and its significance?',\n",
              " '2. How does task decomposition work in the context of problem-solving?',\n",
              " '3. What are the steps involved in breaking down a complex task through task decomposition?',\n",
              " '4. Can you provide real-world examples that illustrate the use of task decomposition?',\n",
              " '5. What is the role of task decomposition in optimizing workflow efficiency?']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_queries.invoke(\"What is Task Decomposition\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
      "metadata": {
        "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
        "outputId": "71d7ccd9-6ade-4555-f732-3f2156b8f97f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Vasanth\\anaconda\\envs\\markdown-validation-crew\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6e74e8-ddae-4165-9e4b-0022ac125194",
      "metadata": {
        "id": "af6e74e8-ddae-4165-9e4b-0022ac125194",
        "outputId": "7f79e0f3-b189-49eb-fc8e-2a6c4cbcb935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down a complex task into smaller and more manageable sub-tasks. This can be achieved through various methods, such as using simple prompting techniques like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", employing task-specific instructions, or incorporating human inputs.\\n\\nOne popular prompting technique is Chain of Thought (CoT), which instructs the model to \"think step by step\" and decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and sheds light on the model\\'s thinking process.\\n\\nAn extension of CoT is Tree of Thoughts, which explores multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\\nTask decomposition is essential for LLM agents as it enables them to handle complex tasks effectively and efficiently.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatGroq(temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef",
      "metadata": {
        "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef"
      },
      "source": [
        "## Part 6: RAG-Fusion\n",
        "\n",
        "Flow:\n",
        "\n",
        "![Screenshot 2024-02-12 at 12.41.36 PM.png](attachment:0bc49f5b-c338-4cd4-ac04-8744994e0e81.png)\n",
        "\n",
        "\n",
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e",
      "metadata": {
        "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion: Related\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9781b40c-c408-42f4-ae14-cd11be513b63",
      "metadata": {
        "id": "9781b40c-c408-42f4-ae14-cd11be513b63"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatGroq(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd4d422",
      "metadata": {
        "id": "8dd4d422"
      },
      "source": [
        "## RRF Explanation\n",
        "\n",
        "#### Question Rankings\n",
        "\n",
        "- **Question A**:\n",
        "  1. Doc1\n",
        "  2. Doc4\n",
        "  3. Doc3\n",
        "  4. Doc2\n",
        "\n",
        "- **Question B**:\n",
        "  1. Doc3\n",
        "  2. Doc1\n",
        "  3. Doc2\n",
        "  4. Doc4\n",
        "\n",
        "- **Question C**:\n",
        "  1. Doc4\n",
        "  2. Doc3\n",
        "  3. Doc1\n",
        "  4. Doc2\n",
        "\n",
        "### Rank Positions\n",
        "\n",
        "- **Doc1**:\n",
        "  - Question A rank: 1\n",
        "  - Question B rank: 2\n",
        "  - Question C rank: 3\n",
        "\n",
        "- **Doc2**:\n",
        "  - Question A rank: 4\n",
        "  - Question B rank: 3\n",
        "  - Question C rank: 4\n",
        "\n",
        "- **Doc3**:\n",
        "  - Question A rank: 3\n",
        "  - Question B rank: 1\n",
        "  - Question C rank: 2\n",
        "\n",
        "- **Doc4**:\n",
        "  - Question A rank: 2\n",
        "  - Question B rank: 4\n",
        "  - Question C rank: 1\n",
        "\n",
        "### Reciprocal Rank Fusion Calculation\n",
        "\n",
        "Using `k = 60`:\n",
        "\n",
        "#### Doc1\n",
        "- Reciprocal Rank (Question A): `1 / (60 + 1) = 1 / 61`\n",
        "- Reciprocal Rank (Question B): `1 / (60 + 2) = 1 / 62`\n",
        "- Reciprocal Rank (Question C): `1 / (60 + 3) = 1 / 63`\n",
        "- **RRF(Doc1)**: `1 / 61 + 1 / 62 + 1 / 63 ≈ 0.0487`\n",
        "\n",
        "#### Doc2\n",
        "- Reciprocal Rank (Question A): `1 / (60 + 4) = 1 / 64`\n",
        "- Reciprocal Rank (Question B): `1 / (60 + 3) = 1 / 63`\n",
        "- Reciprocal Rank (Question C): `1 / (60 + 4) = 1 / 64`\n",
        "- **RRF(Doc2)**: `1 / 64 + 1 / 63 + 1 / 64 ≈ 0.0469`\n",
        "\n",
        "#### Doc3\n",
        "- Reciprocal Rank (Question A): `1 / (60 + 3) = 1 / 63`\n",
        "- Reciprocal Rank (Question B): `1 / (60 + 1) = 1 / 61`\n",
        "- Reciprocal Rank (Question C): `1 / (60 + 2) = 1 / 62`\n",
        "- **RRF(Doc3)**: `1 / 63 + 1 / 61 + 1 / 62 ≈ 0.0487`\n",
        "\n",
        "#### Doc4\n",
        "- Reciprocal Rank (Question A): `1 / (60 + 2) = 1 / 62`\n",
        "- Reciprocal Rank (Question B): `1 / (60 + 4) = 1 / 64`\n",
        "- Reciprocal Rank (Question C): `1 / (60 + 1) = 1 / 61`\n",
        "- **RRF(Doc4)**: `1 / 62 + 1 / 64 + 1 / 61 ≈ 0.0484`\n",
        "\n",
        "### Final Ranking\n",
        "\n",
        "Based on the RRF scores:\n",
        "\n",
        "1. **Doc1**: `≈ 0.0487`\n",
        "2. **Doc3**: `≈ 0.0487`\n",
        "3. **Doc4**: `≈ 0.0484`\n",
        "4. **Doc2**: `≈ 0.0469`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
      "metadata": {
        "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
        "outputId": "40b63f72-0f88-4fd0-eb8c-94ff031b792f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
      "metadata": {
        "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
        "outputId": "ccface5d-0750-481a-ed6e-40f149056514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Task decomposition for LLM (large language model) agents refers to the process of breaking down large tasks into smaller, manageable subgoals. This allows the agent to handle complex tasks more efficiently. Task decomposition can be done using simple prompting techniques, task-specific instructions, or with human inputs. LLMs like HuggingGPT use task planning and parsing to decompose user requests into multiple tasks with associated attributes. However, there are challenges in long-term planning and task decomposition, such as the limited context length and the reliability of the natural language interface.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c812d3-4d91-4634-8301-0b68be88a887",
      "metadata": {
        "id": "94c812d3-4d91-4634-8301-0b68be88a887"
      },
      "source": [
        "## Part 7: Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82fac99-58dc-4bb9-84e6-51180db855ad",
      "metadata": {
        "id": "f82fac99-58dc-4bb9-84e6-51180db855ad"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4",
      "metadata": {
        "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(temperature=0)\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
      "metadata": {
        "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
        "outputId": "473dc45b-530a-4ba3-9129-19f40afa2cb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1. \"Main components of a language model in autonomous agent systems\"',\n",
              " '2. \"Role of natural language understanding in LLM-powered autonomous agents\"',\n",
              " '3. \"How does natural language generation contribute to autonomous agent systems with LLMs?\"']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19543d04-ff31-4774-b89c-9d31f5a28fc9",
      "metadata": {
        "id": "19543d04-ff31-4774-b89c-9d31f5a28fc9"
      },
      "source": [
        "### Answer recursively  \n",
        "\n",
        "![Screenshot 2024-02-18 at 1.55.32 PM.png](attachment:9a9685de-051f-48fa-b68f-2b1f85344cdf.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa",
      "metadata": {
        "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20bf0d4-f567-4451-834d-a07190a3185e",
      "metadata": {
        "id": "a20bf0d4-f567-4451-834d-a07190a3185e"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "# llm\n",
        "llm = ChatGroq(temperature=0)\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions:\n",
        "\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
      "metadata": {
        "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
        "outputId": "ac934146-5060-453d-c45f-e88a15fe57cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Natural language generation (NLG) contributes to autonomous agent systems with LLMs (large language models) by enabling the agent to process, comprehend, and generate human-like language. This is crucial for the agent to communicate effectively with users and external components, as well as to handle complex tasks through subgoal decomposition and self-reflection.\\n\\nThe LLM at the core of the autonomous agent system is responsible for natural language generation, and it can perform various NLG tasks such as text generation, translation, summarization, and question-answering. The LLM can generate natural language outputs based on the given goals and past actions, and it can learn from its mistakes through self-reflection and self-criticism.\\n\\nMoreover, the LLM can generate natural language inputs for external components such as memory and tools, and it can process natural language inputs from the user and external components to make decisions and take actions. The natural language interface between the LLM and external components relies on natural language understanding (NLU) and NLG, and it can be enhanced through parsing model output and handling formatting errors and rebellious behavior.\\n\\nTherefore, natural language generation is a key component of autonomous agent systems with LLMs, as it enables the agent to communicate effectively, handle complex tasks, learn from past actions, and interact with external components through a natural language interface. However, there are also challenges in natural language generation for autonomous agents, such as the limited context capacity, long-term planning and task decomposition, and the reliability of the natural language interface.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0fa2e4-d4f1-42fc-a1ad-8eaeb05a0d3e",
      "metadata": {
        "id": "eb0fa2e4-d4f1-42fc-a1ad-8eaeb05a0d3e"
      },
      "source": [
        "### Answer individually\n",
        "\n",
        "![Screenshot 2024-02-18 at 2.00.59 PM.png](attachment:e24502d7-f641-4262-a326-da1636822fa2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297425fa-975b-4599-9b9e-a11139b99140",
      "metadata": {
        "id": "297425fa-975b-4599-9b9e-a11139b99140",
        "outputId": "d5fa798e-25eb-4b3c-ba2c-524f851755e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Vasanth\\anaconda\\envs\\markdown-validation-crew\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Answer each sub-question individually\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# RAG prompt\n",
        "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
        "    \"\"\"RAG on each sub-question\"\"\"\n",
        "\n",
        "    # Use our decomposition /\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "    # Initialize a list to hold RAG chain results\n",
        "    rag_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "\n",
        "        # Use retrieved documents and sub-question in RAG chain\n",
        "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
        "                                                                \"question\": sub_question})\n",
        "        rag_results.append(answer)\n",
        "\n",
        "    return rag_results,sub_questions\n",
        "\n",
        "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
        "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8631dda-bbcd-437c-81b3-5db7abb831f9",
      "metadata": {
        "id": "b8631dda-bbcd-437c-81b3-5db7abb831f9",
        "outputId": "48bc711b-d824-4614-ed70-c9c8fe366955"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The main components of an LLM-powered autonomous agent system include the language model (LLM) and the system message. The LLM, which can make decisions independently, serves as the brain for natural language understanding, enabling complex task handling through subgoal decomposition, learning from past actions through reflection and refinement, and maintaining memory. Meanwhile, the system message defines the AI's name, description, and goals. Additionally, natural language generation acts as the main controller, allowing the system to handle complex tasks, decompose subgoals, and provide self-reflection for continuous learning and improvement. However, there are common challenges across these components, such as limited context length, difficulties in long-term planning and task decomposition, and reliability issues with the natural language interface.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def format_qa_pairs(questions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "context = format_qa_pairs(questions, answers)\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Here is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":context,\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6126bebb-94e5-48ef-9a17-6a315ed0a596",
      "metadata": {
        "id": "6126bebb-94e5-48ef-9a17-6a315ed0a596"
      },
      "source": [
        "## Part 8: Step Back\n",
        "\n",
        "![Screenshot 2024-02-12 at 1.14.43 PM.png](attachment:715e11dc-7730-4f51-8469-b7f0b299ac9e.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d74f9f2-543d-4e41-b90b-7bb527eca1d9",
      "metadata": {
        "id": "1d74f9f2-543d-4e41-b90b-7bb527eca1d9"
      },
      "outputs": [],
      "source": [
        "# Few Shot Examples\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "# We now transform these to example messages\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot_prompt,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cba100d-167f-4392-8f58-88729d3e4ce9",
      "metadata": {
        "id": "5cba100d-167f-4392-8f58-88729d3e4ce9",
        "outputId": "8618fd9b-1c16-4224-d267-b63e643b3f0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"How is a large language model agent's task broken down? \\n\\nor\\n\\nWhat are the steps involved in completing a task for a large language model agent?\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_queries_step_back = prompt | ChatGroq(temperature=0) | StrOutputParser()\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "generate_queries_step_back.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999445b0-d8a0-4208-9bb6-38610667a00b",
      "metadata": {
        "id": "999445b0-d8a0-4208-9bb6-38610667a00b",
        "outputId": "6ef118e6-3004-468c-b17f-a561ba187889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Task decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, manageable subgoals. This allows the agent to handle complex tasks more efficiently and effectively. There are several methods for task decomposition in LLM agents, including:\\n\\n1. Simple prompting: The LLM is prompted with simple instructions, such as \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to decompose a task.\\n2. Task-specific instructions: The LLM is provided with specific instructions for a particular task, such as \"Write a story outline.\" for writing a novel.\\n3. Human inputs: The LLM can also receive task decomposition input from human users.\\n\\nAdditionally, there are advanced techniques such as Chain of Thought (CoT) and Tree of Thoughts (ToT) that enhance model performance on complex tasks by decomposing hard tasks into smaller and simpler steps, and exploring multiple reasoning possibilities at each step, respectively.\\n\\nIn a LLM-powered autonomous agent system, task decomposition is a crucial component that works in conjunction with other components such as planning, reflection and refinement, and memory to enable the agent to effectively handle complex tasks and learn from past mistakes. However, there are challenges in long-term planning and task decomposition, such as the restricted context capacity that limits the inclusion of historical information and detailed instructions, and the reliability of natural language interface, which may result in formatting errors and occasional rebellious behavior in the model outputs.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Response prompt\n",
        "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        # Retrieve context using the normal question\n",
        "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
        "        # Retrieve context using the step-back question\n",
        "        \"step_back_context\": generate_queries_step_back | retriever,\n",
        "        # Pass on the question\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "    }\n",
        "    | response_prompt\n",
        "    | ChatGroq(temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d0e558-4abe-42e4-a33a-2b93692f5fab",
      "metadata": {
        "id": "63d0e558-4abe-42e4-a33a-2b93692f5fab"
      },
      "source": [
        "## Part 9: HyDE\n",
        "\n",
        "![Screenshot 2024-02-12 at 1.12.45 PM.png](attachment:1982149e-720b-426e-a1ab-8d96f6616b5a.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2902575-bbbb-41a9-835b-9a24dc08261b",
      "metadata": {
        "id": "c2902575-bbbb-41a9-835b-9a24dc08261b",
        "outputId": "872fe8ca-e06f-4b73-972e-e29c0658b9bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Task decomposition is a crucial aspect of building Large Language Model (LLM) agents that can effectively perform complex tasks. It involves breaking down a complex task into smaller, manageable subtasks that the LLM agent can handle more efficiently. This process is similar to the way humans approach complex problems by dividing them into smaller, more manageable parts.\\n\\nIn the context of LLM agents, task decomposition enables the agent to focus on one subtask at a time, reducing the cognitive load and increasing the chances of success. Each subtask is defined in a way that the LLM agent can understand and execute, often involving natural language instructions.\\n\\nFor example, consider a task where an LLM agent is required to write a scientific paper. The agent can decompose this task into several subtasks, such as:\\n\\n1. Understanding the research question\\n2. Conducting a literature review\\n3. Developing a thesis statement\\n4. Outlining the paper\\n5. Writing the introduction\\n6. Writing the body\\n7. Writing the conclusion\\n8. Proofreading and editing\\n\\nEach of these subtasks can be further broken down into even smaller tasks, depending on the complexity of the overall task. By decomposing the task in this way, the LLM agent can focus on one subtask at a time, reducing the risk of errors and increasing the efficiency of the process.\\n\\nTask decomposition is particularly important for LLM agents because of their limited capacity to process and retain information. By breaking down tasks into smaller parts, the agent can focus on the relevant information for each subtask, reducing the risk of confusion and increasing the chances of success.\\n\\nIn summary, task decomposition is a critical technique for building LLM agents that can effectively perform complex tasks. It involves breaking down a complex task into smaller, manageable subtasks that the LLM agent can understand and execute, increasing the efficiency and success rate of the agent.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# HyDE document genration\n",
        "template = \"\"\"Please write a scientific paper passage to answer the question\n",
        "Question: {question}\n",
        "Passage:\"\"\"\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "generate_docs_for_retrieval = (\n",
        "    prompt_hyde | ChatGroq(temperature=0) | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "generate_docs_for_retrieval.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47587bb-23db-42a0-b087-beef9e95308b",
      "metadata": {
        "id": "d47587bb-23db-42a0-b087-beef9e95308b",
        "outputId": "ab236c92-98a2-4899-be51-852e24055497"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
              " Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
              " Document(page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
              " Document(page_content=\"Each element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieve\n",
        "retrieval_chain = generate_docs_for_retrieval | retriever\n",
        "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
        "retireved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604fcc36-a1d7-4096-99b5-50db30950fc5",
      "metadata": {
        "id": "604fcc36-a1d7-4096-99b5-50db30950fc5",
        "outputId": "b2eb358e-f97a-48d7-aef5-31e8fa33b159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Task decomposition for LLM (large language model) agents refers to the process of breaking down large tasks into smaller, manageable subgoals. This allows the agent to handle complex tasks more efficiently. Task decomposition can be done using simple prompting techniques, task-specific instructions, or with human inputs. The document also mentions a technique called Tree of Thoughts, which extends the concept of task decomposition by exploring multiple reasoning possibilities at each step, creating a tree structure. The search process in Tree of Thoughts can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier or majority vote.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270dbd23-6f16-4827-a401-75889f0e3506",
      "metadata": {
        "id": "270dbd23-6f16-4827-a401-75889f0e3506"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}